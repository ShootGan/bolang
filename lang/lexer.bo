module lexer

imports ../std/io
imports ../std/strings
imports ../std/chars

type Token {null | TokOp:string | TokInt:i64 | TokIdent:string | TokKeyword:string }

let keywords  = ["let", "while", "fn", "true", "false", "null", "if", "else"]
let operators = ["+", "-", "*", "/", "%", "=", "+=", "-=", "*=", "/=", "%="]


fn parseKeyword(str string) (Token, string)
    let (i, k) = (0, 0)

    while i < len(str) && isAlpha(str[i])
        i = i + 1

    let word = str[..i]
    while k < len(keywords)
        if strEqual(word, keywords[k])
            return (TokKeyword(&:string(word)), str[i..])
        k = k + 1

    return (null, str)


fn parseIdent(str string) (Token, string)
    let i = 0

    if i >= len(str) || !isAlpha(str[i])
        return (null, str)

    while i < len(str) && (isDigit(str[i]) || isAlpha(str[i]))
        i = i + 1

    return (TokIdent(&str[..i]), str[i..])


fn parseOperator(str string) (Token, string)
    let (i, j, max) = (0, 0, 0)

    while i < len(operators)
        if isPrefix(operators[i], str) && len(operators[i]) > max
            max = len(operators[i])
            j = i
        i = i + 1
    
    switch max
        0; return (null, str)
        n; return (TokOp(&:string(operators[j])),  str[n..])


fn parseWhite(str string) string
    let i = 0
    while i < len(str) && (str[i] == ' ' || str[i] == '\t')
        i = i + 1

    return str[i..]


fn parseToken(str string) (Token, string)
    switch parseKeyword(str)
        (null, _) ;
        (tok, ss) ; return (tok, parseWhite(ss))

    switch parseIdent(str)
        (null, _) ;
        (tok, ss) ; return (tok, parseWhite(ss))

    switch parseOperator(str)
        (null, _) ;
        (tok, ss) ; return (tok, parseWhite(ss))

    return (null, str)




            
    
